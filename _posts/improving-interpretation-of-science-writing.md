---
title: Improving Interpretation of Science Writing
author: Paul Meinshausen
layout: post
permalink: /2013/03/14/improving-interpretation-of-science-writing/
jabber_published:
  - 1363285456
email_notification:
  - 1363285457
publicize_twitter_user:
  - SchaunW
categories:
  - Analysis
  - Dissemination
  - Interpretation
tags:
  - Clash of Civilizations
  - Interpretation
  - jargon
  - Jonah Lehrer
  - MIT Technology Review
  - popular science
  - science communication
  - Science writing
---
<span style="color: #000000;">Stories of recent </span><span style="color: #000080;"><span style="text-decoration: underline;"><a href="http://www.nytimes.com/2012/10/06/opinion/fraud-in-the-scientific-literature.html?_r=0">fraudulent science</a></span></span><span style="color: #000000;"> seem </span><span style="color: #000080;"><span style="text-decoration: underline;"><a href="http://www.nytimes.com/2012/04/17/science/rise-in-scientific-journal-retractions-prompts-calls-for-reform.html?_r=1&hpw=&pagewanted=all">uncomfortably common</a></span></span><span style="color: #000000;">. In many of those cases the scientists are blamed, and </span><span style="color: #000080;"><span style="text-decoration: underline;"><a href="http://retractionwatch.wordpress.com/2013/03/07/wash-u-psychologist-sheds-light-on-inquiry-against-former-psychology-grad-student/">rightly so</a></span></span><span style="color: #000000;">. Sometimes criticisms identify more systemic problems like </span><span style="color: #000080;"><span style="text-decoration: underline;"><a href="http://www.talyarkoni.org/blog/2013/03/12/the-truth-is-not-optional-five-bad-reasons-and-one-mediocre-one-for-defending-the-status-quo/">current scientific practice</a></span></span><span style="color: #000000;">, or </span><span style="color: #000080;"><span style="text-decoration: underline;"><a href="http://iai.asm.org/content/80/3/897.full">scientific institutions</a></span></span><span style="color: #000000;"> like the NSF or a university, or academia in general. Blame is also often laid on pop science and </span><span style="color: #000080;"><span style="text-decoration: underline;"><a href="http://blogs.discovermagazine.com/neuroskeptic/2013/03/01/were-all-jonah-lehrer-except-me/#.UT6CgVpxte4">the popular science writers</a></span></span><span style="color: #000000;"> who try to tell </span><span style="color: #000080;"><span style="text-decoration: underline;"><a href="http://evolvingthoughts.net/2013/02/drama-journalism-and-science/">a counterintuitive and interesting story</a></span></span><span style="color: #000000;">, or who are under pressure to write frequently and under a deadline.</span>

<span style="color: #000000;">All of these are valid targets of criticism. But not enough attention is paid to the scientifically-inclined and interested public who are supposedly the victims of the fraudulent findings and stories. Given the incredible specialization of contemporary science, everyone is an untrained and naive reader in some, if not most, areas of science. But despite technical ignorance, the activity of reading about science can be done usefully and well, and doing it well means maintaining a general attitude of scientific doubt.</span><!--more-->

<span style="color: #000000;">In general, few things are as important as maintaining a general skepticism. When it comes to science I&#8217;m a short-term pessimist and a long-term optimist. Science has made immense contributions to the world, and I think it will continue to do so. But on net, any particular paper is sadly likely to be </span><span style="color: #000080;"><span style="text-decoration: underline;"><a href="http://www.plosmedicine.org/article/info:doi/10.1371/journal.pmed.0020124">invalid</a></span></span><span style="color: #000000;">. Richard Feynman </span><span style="color: #000080;"><span style="text-decoration: underline;"><a href="http://www.brainpickings.org/index.php/2013/03/06/richard-feynman-responsibility-of-scientists/">put it like this</a></span></span><span style="color: #000000;">:</span>

> * <span style="color: #000000;">&#8220;When a scientist doesn</span><span style="color: #000000;">’</span><span style="color: #000000;">t know the answer to a problem, he is ignorant. When he has a hunch as to what the result is, he is uncertain. And when he is pretty darn sure of what the result is going to be, he is in some doubt. We have found it of paramount importance that in order to progress we must recognize the ignorance and leave room for doubt.&#8221;</span>*

<span style="color: #000000;">If what you&#8217;re reading packages the world into neat little explanatory presents, and then hands them to you across the page with a bow on top, then it makes sense to stop and think: &#8220;&#8230;nah, probably not.&#8221;</span>

<span style="color: #000000;">Take </span><span style="color: #000080;"><span style="text-decoration: underline;"><a href="http://www.slate.com/articles/health_and_science/science/2012/08/jonah_lehrer_plagiarism_in_wired_com_an_investigation_into_plagiarism_quotes_and_factual_inaccuracies_.3.html">Jonah Lehrer for example</a></span></span><span style="color: #000000;">. Lots of attention was paid to the </span><span style="color: #000080;"><span style="text-decoration: underline;"><a href="http://thecognitiveturn.com/2012/08/09/brilliance-at-gunpoint-jonah-lehrer-creativity-and-the-pressures-of-intellectual-performance/">writing deadlines</a></span></span><span style="color: #000000;"> he was </span><span style="color: #000080;"><span style="text-decoration: underline;"><a href="http://www.slate.com/articles/life/culturebox/2012/06/jonah_lehrer_self_plagiarism_the_new_yorker_staffer_stopped_being_a_writer_and_became_an_idea_man_.html">expected to meet</a></span></span><span style="color: #000000;">. And to his urge to tell a good story. Less attention was paid to the notion that he actually </span><span style="color: #000080;"><span style="text-decoration: underline;"><a href="http://blog.chabris.com/2013/02/what-has-been-forgotten-about-jonah.html">wasn&#8217;t that great of a science writer</a></span></span><span style="color: #000000;">. Whatever the reasons for his apparently deliberate unethical behavior, they wouldn&#8217;t have been as interesting if he hadn&#8217;t been as widely read as he was. And he shouldn&#8217;t have been as widely read as he was. That he was is to some degree a problem of skeptical reading. So it seems worthwhile to try to develop ways of systematically reading critically and skeptically.</span>

<span style="color: #000000;">I&#8217;m taking Gary King&#8217;s </span><span style="color: #000080;"><span style="text-decoration: underline;"><a href="http://projects.iq.harvard.edu/gov2001/">Gov 2001 class</a></span></span><span style="color: #000000;"> this semester. In a recent lecture he mentioned </span><span style="color: #000080;"><span style="text-decoration: underline;"><a href="http://gking.harvard.edu/files/abs/making-abs.shtml">a paper he&#8217;d written</a></span></span><span style="color: #000000;"> about improving the interpretation and presentation of statistical analyses in social science. After class I went and read the paper. In the introduction, he lays out three steps for presenting any statistical procedure: </span>

> *<span style="color: #000000;">&#8220;More specifically, we show how to convert the raw results of any statistical procedure into expressions that (1) convey numerically precise estimates of the quantities of greatest substantive interest, (2) include reasonable measures of uncertainty about those estimates, and (3) require little specialized knowledge to understand.&#8221;</span>*

<span style="color: #000000;">Transformed into questions these three steps form a great guide for critically reading science writing:</span>

1.   <span style="color: #000000;">Does the article present numerically precise estimates of the quantities of greatest substantive interest?</span>
2.  <span style="color: #000000;">Does the article include reasonable measures of uncertainty about those estimates?</span>
3.  <span style="color: #000000;">Does the article require little specialized knowledge to understand?</span>

<span style="color: #000000;">I&#8217;ll demonstrate using </span><span style="color: #000080;"><span style="text-decoration: underline;"><a href="http://www.technologyreview.com/view/512116/global-e-mail-patterns-reveal-clash-of-civilizations/#.UTY6J23CXyo.twitter">a blog post</a></span></span><span style="color: #000000;"> about a study that I read in the MIT Tech Review recently. The post was about a paper titled </span><span style="color: #000080;"><span style="text-decoration: underline;"><a href="http://arxiv.org/pdf/1303.0045v1.pdf">&#8220;The Mesh of Civilizations and International Email Flows&#8221;</a></span></span><span style="color: #000000;">. The paper presents an interesting dataset, though I don&#8217;t understand why the authors decided to think about it in the terms of a vague collection of strange ideas like Huntington&#8217;s &#8220;Clash of Civilizations thesis&#8221;. But whatever the merits and demerits of the paper, the blog post about it is a lesson in poor science writing. The above questions help expose that. </span>

<span style="color: #000000;"><strong>Numerical precision</strong></span>  
<span style="color: #000000;">A big part of science is the generation of questions and ideas that drive data collection, management, analysis and presentation. But questions and ideas by themselves are not yet &#8220;science&#8221; or scientific results. If a post is about ideas, then you can set evidence aside and just enjoy the intellectual thrill of thinking creatively. But if it purports to describe scientific results, then the work it&#8217;s describing must involve systematic (usually statistical) analysis of numbers that represent some part of the world. When numbers are used, they should be precise. For example, when an analysis shows that a particular drug helps people lose weight, then the write-up should say how much weight it helps people lose. When an analysis reports that a particular policy has reduced crime in a city, then the description should say by how much crime was reduced, and that number should be substantively meaningful.</span>

<span style="color: #000000;">The post does not present numerically precise estimates of the quantities of interest. Instead we get rambling and vague assertions. When a writer says something like &#8220;For example, a common border between two countries actually reduces the communication density between them&#8221;, she or he should report the size of that reduction. If the size isn&#8217;t reported, then it&#8217;s a worthwhile suspicion that either the reduction was so small that it&#8217;s not interesting, or that the writer doesn&#8217;t know, in which case what he or she has to say about the research isn&#8217;t very credible. </span>

**Reasonable uncertainty**  
<span style="color: #000000;">If numbers are going to be used, then they should be reported precisely. However, </span><span style="color: #000080;"><span style="text-decoration: underline;"><a href="http://www.radiolab.org/2009/jun/15/">uncertainty</a></span></span><span style="color: #000000;"> is an inherent </span><span style="color: #000080;"><span style="text-decoration: underline;"><a href="http://en.wikipedia.org/wiki/Stochastic">part of the world</a></span></span><span style="color: #000000;">, and precision should always be accompanied by a reasonable measure of that uncertainty. When results can be reported numerically (and if they can&#8217;t, then the research isn&#8217;t ready for public consumption, at least not in the guise of science) then uncertainty also can and should be reported numerically. If a common border reduces the communication density between countries, then readers should be told that that reduction is plus or minus some other number. </span>

<span style="color: #000000;">The post doesn&#8217;t come close to including reasonable measures of uncertainty about those estimates. In fact, not only does it not present reasonable measures of uncertainty, it doesn&#8217;t convey any kind of uncertainty. Instead we get misleadingly strong assertions about what the research &#8220;clearly reflects&#8221;.</span>

<span style="color: #000000;"><strong>Specialized knowledge</strong></span>  
<span style="color: #000000;">Methodological jargon blurs messy and inconvenient details, since confusion over particular terms stalls more substantive questions and debate. Jargon serves a purpose, but it rarely belongs in writing intended for a general, non-specialist audience. </span>

<span style="color: #000000;">The post isn&#8217;t terrible in its use of jargon (thankfully it never calls a finding &#8220;statistically significant&#8221;). But while the author splurges on the space and words used to describe Huntington&#8217;s thesis (which was hardly the substance of the paper) where a link or two would have served just as well or better, it does use jargon to blur over key details of the research. After noting the problems with the data and that the hard work was in cleaning it, the author describes that work as &#8220;a comprehensive &#8220;rescaling&#8221; of the data to account for these effects.&#8221; The paper&#8217;s methods are summarized as that the authors &#8220;crunched the data using a number of community detection algorithms&#8221;. That latter description is an example of using jargon to make a simple and relatively meaningless statement seem instead an impressive outline of a complicated and complex process and procedure. If they had instead said something with the exact same meaning but in plain language like, &#8220;the authors used data and a </span><span style="color: #000080;"><span style="text-decoration: underline;"><a href="http://en.wikipedia.org/wiki/Algorithm">step-by-step procedure for calculation</a></span></span><span style="color: #000000;">&#8220;, you&#8217;d have to wonder what information they were providing. They could have said it even simpler and with just as much meaning with the statement &#8220;they did an analysis&#8221;. Not saying much.</span>

<span style="color: #000000;">I&#8217;d actually extend the original step here a little for reading purposes. Avoiding unnecessary technical and methodological jargon is important for communicating to a general audience. But it&#8217;s useful even for experts as a way of avoiding lazy or sloppy thinking. In his book &#8220;Surely You&#8217;re Joking Mr. Feynman&#8221;, Richard Feynman describes his experience discovering the poor quality of science education in Brazil at the time. His account of his talk to an audience of scientists in Brazil makes this point wonderfully:  </span>

> *<span style="color: #000000;">&#8220;I have discovered something else,&#8221; I continued. &#8220;By flipping the pages at random, and putting my finger in and reading the sentences on that page, I can show you what&#8217;s the matter &#8211; how it&#8217;s not science, but memorizing, in every circumstance&#8230;So I did it. Brrrrup &#8211; I stuck my finger in, and I started to read: &#8220;Triboluminescence. Triboluminescence is the light emitted when crystals are crushed&#8230;&#8221;</span>*
> 
> *<span style="color: #000000;">I said, &#8220;And there, have you got science? No! You have only told what a word means in terms of other words. You haven&#8217;t told anything about nature &#8211; what crystals produce light when you crush them, why they produce light. Did you see any student go home and try it? He can&#8217;t. &#8220;But if, instead, you were to write, &#8220;When you take a lump of sugar and crush it with a pair of pliers in the dark, you an see a bluish flash. Some other crystals do that too. Nobody knows why. The phenomenon is called &#8220;triboluminescence.&#8221;&#8216; Then someone will go home and try it. Then there&#8217;s an experience of nature.&#8221;</span>*

There&#8217;s a joy to reading about science that few things come close to matching. Perhaps part of that joy is because reading about science is itself a partly scientific endeavor. You start with a question, then you get an answer, and then you question that answer. Good science writing takes you through that process. Poor science writing exudes the gross kind of unquestioning confidence, garishly polished style, and flattering focus on the researcher that&#8217;s become <span style="color: #000080;"><span style="text-decoration: underline;"><a href="http://blogs.hbr.org/haque/2013/03/lets_save_great_ideas_from_the.html">the mark of a TED talk</a></span></span>. But poor science writing is ultimately only a problem if readers themselves can&#8217;t tell the difference.